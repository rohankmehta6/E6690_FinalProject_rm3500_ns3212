---
title: "SLproject"
author: "Nanda_Rohan"
date: "April, 2018"
output: pdf_document
---

```{r }

# setting the directory 
setwd("D:/Me/Columbia University/1-2/Statistical Learning for Biological and Information Systems/Project/T-12_Bitbrains/fastStorage/2013-8")

#indentifying all the files of type "csv" in the directory
temp <- list.files(pattern = "*.csv")

## Loading all the files from the directory to the dataframe
df <- list()

for (i in 1:length(temp))
{
  df[[i]] <- read.csv(file= temp[i],header = TRUE, sep = ";")
}

```

!!--- REPRODUCING ANALYSIS AS DONE IN THE REFERENCE PAPER---!!!


```{r }

library(tidyr)
library(prophet)
library(dplyr)
library(ggplot2)
library(tseries)
library(forecast)

## Reproduction of analysis from paper: Code to show CDF of number of cores available in VMs
cores <- list(1,2,3,4,5,6)

for (i in 1:length(temp))
{
    if (df[[i]][4,2] == 1){
        cores[[1]] = cores[[1]]+1
        } 
    else if (df[[i]][4,2] == 2) {
        cores[[2]] = cores[[2]]+1
        } 
    else if (df[[i]][4,2] == 4) {
        cores[[3]] = cores[[3]]+1
        } 
    else if (df[[i]][4,2] == 8) {
        cores[[4]] = cores[[4]]+1
        }
    else if (df[[i]][4,2] == 16) {
        cores[[5]] = cores[[5]]+1
        }
    else if (df[[i]][4,2] == 32) {
        cores[[6]] = cores[[6]]+1
        }
}


sum.cores <- list(1,2,3,4,5,6)

# Calculating CDF
sum.cores[[1]]=cores[[1]]
for (i in 2:6) {
     sum.cores[[i]] = sum.cores[[i-1]] + cores[[i]]
}

# Plotting CDF
x <- list(1,2,4,8,16,32)
plot(x, sum.cores, xlab = "Number of cores", ylab = "Number of VMs", type="o", pch=22)

```



```{r}
## Reproduction of analysis from paper: Code to show CDF of amount of memory available in VMs
mem <- list(1,2,3,4,5,6,7,8,9,10)

for(i in 1:length(temp)){
     if (df[[i]][4,6] > 510000000 && df[[i]][4,6] < 520000000){
         mem[[10]] = mem[[10]]+1
     } else if (df[[i]][4,6] > 250000000 && df[[i]][4,6] < 260000000){
         mem[[9]] = mem[[9]]+1
     } else if (df[[i]][4,6] > 120000000 && df[[i]][4,6] < 130000000){
         mem[[8]] = mem[[8]]+1
     } else if (df[[i]][4,6] > 60000000 && df[[i]][4,6] < 70000000){
         mem[[7]] = mem[[7]]+1
     } else if (df[[i]][4,6] > 30000000 && df[[i]][4,6] < 40000000){
         mem[[6]] = mem[[6]]+1
     } else if (df[[i]][4,6] > 10000000 && df[[i]][4,6] < 20000000){
         mem[[5]] = mem[[5]]+1
     } else if (df[[i]][4,6] > 8000000 && df[[i]][4,6] < 9000000){
         mem[[4]] = mem[[4]]+1
     } else if (df[[i]][4,6] > 40000000 && df[[i]][4,6] < 50000000){
         mem[[3]] = mem[[3]]+1
     } else if (df[[i]][4,6] > 20000000 && df[[i]][4,6] < 30000000){
         mem[[2]] = mem[[2]]+1
     } else if (df[[i]][4,6] > 10000000 && df[[i]][4,6] < 20000000){
         mem[[1]] = mem[[1]]+1
     }
}

x <- list(1,2,4,8,16,32,64,128,256,512)
sum.mem <- list(1,2,3,4,5,6,7,8,9,10)

# Calculating CDF
sum.mem[[1]] = mem[[1]]
for (i in 2:10) {
      sum.mem[[i]] = sum.mem[[i-1]] + mem[[i]]
}
 
# Plotting CDF 
plot(x, sum.mem, xlab = "Memory Available", ylab = "Number of VMs", type="o", pch=22)


```

```{r }
# Renaming columns for easy usage as they initially had lot of special characters

for(i in 1:length(temp)){
  names(df[[i]])[2] <- "CPUcores"
  }
for(i in 1:length(temp)){
     names(df[[i]])[3] <- "CPUcapacity"
 }
for(i in 1:length(temp)){
     names(df[[i]])[5] <- "CPUusagePERC"
 }
for(i in 1:length(temp)){
     names(df[[i]])[6] <- "MEMcapacity"
 }
for(i in 1:length(temp)){
     names(df[[i]])[7] <- "MEMusage"
}
for(i in 1:length(temp)){
     names(df[[i]])[8] <- "DiskReadTP"
 } 
for(i in 1:length(temp)){
     names(df[[i]])[9] <- "DiskWriteTP"
 }
for(i in 1:length(temp)){
     names(df[[i]])[10] <- "NETrecvTP"
 }
for(i in 1:length(temp)){
     names(df[[i]])[11] <- "NETtransTP"
}
for(i in 1:length(temp)){
     names(df[[i]])[4] <- "CPUusage"
}
for(i in 1:length(temp)){
     names(df[[i]])[1] <- "timestamp"
}

```



!!!--- FROM HERE ONWARD, WE ARE CREATING FOUR TIME SERIES; EACH FOR %CPU USAGE, %MEMORY USAGAE, DISK I/O THROUGHPUT, AND NETWORK I/O THROUGHPUT ---!!!


```{r }

# % CPU usage averaged across all VMs in the data centre for 30 days (August 2013 - september 2013)

i=0
j=0
n1=0
sum1=0
avg1 <- list()
for(j in 1:8600){
     n1=0
     sum1=0
     for(i in 1:length(temp)){
         if (!is.na(df[[i]][j,5])) {
             sum1 = sum1 + df[[i]][j,5]
             n1 = n1+1
         }
     }
     avg1[j]=sum1/n1
 }

plot(1:8600,avg1, xlab = "5*X minutes", ylab = "Total Average CPU usage")


```

```{r}

# Network throughput (incoming + outgoing) averaged across all VMs in the data centre for 30 days (August 2013 - september 2013)

i=0
j=0
n2=0
sum2=0
avg2 <- list()
for(j in 1:8600){
     n2=0
     sum2=0
     for(i in 1:length(temp)){
          if ((!is.na(df[[i]][j,10])) && (!is.na(df[[i]][j,11]))) {
             sum2 = sum2 + df[[i]][j,10]+ df[[i]][j,11]
             n2 = n2+1
         }
     }
     avg2[j]=sum2/n2
 }

plot(1:8600,avg2, xlab = "5*X minutes", ylab = "Total Network BW throughput")


#plot without the outlier

for (i in 1:8600) {
  if (avg2[i]>3000) {
     avg2[i]=0
  }}

plot(1:8600,avg2, xlab = "5*X minutes", ylab = "Total Network BW throughput")

```

```{r }

# Disk Throughput (read + write) averaged across all VMs in the data centre for 30 days (August 2013 - september 2013)

i=0
j=0
n3=0
sum3=0
avg3 <- list()
for(j in 1:8600){
     n3=0
     sum3=0
     for(i in 1:length(temp)){
         if ((!is.na(df[[i]][j,8])) && (!is.na(df[[i]][j,9]))) {
             sum3 = sum3 + df[[i]][j,8] + df[[i]][j,9]
             n3 = n3+1
         }
     }
     avg3[j]=sum3/n3
 }
 
plot(1:8600,avg3, xlab = "5*X minutes", ylab = "Total Disk Usage")


```


```{r }

# % memory usage averaged across all VMs in the data centre for 30 days (August 2013 - september 2013)

i=0
j=0
n4=0
sum4=0
avg4 <- list()

for(j in 1:8600){
   n4=0
   sum4=0
   for(i in 1:length(temp)){
       if ((!is.na(df[[i]][j,6])) && (!is.na(df[[i]][j,7])) && (df[[i]][j,6]!=0)) {
           sum4 = sum4 + (df[[i]][j,7] / df[[i]][j,6])*100
           n4 = n4+1
       }
   }
     avg4[j]=sum4/n4
}

plot(1:8600,avg4, xlab = "5*X minutes", ylab = "Total % memory accessed")

# removing outliers

for (i in 1:8600) {
    if (avg4[i]>100) {
        avg4[i]=0
    }}


plot(1:8600,avg4, xlab = "5*X minutes", ylab = "Total % memory accessed")


```


!!!... FROM HERE ONWARD, WE ARE FITTING THREE DIFFERENT MODELS TO EACH OF THE ABOVE CALCULATED FOUR TIME SERIES WORKLOADS ...!!!



```{r}
#1. % CPU USAGE

# making a new Dataframe for AVG CPU usage and x5mins for training
tempavg1=avg1
cpup = data.frame(Reduce(rbind, tempavg1))
names(cpup) <- "avgCPUperc"
cpup <- cbind(cpup, 1:8600)
names(cpup)[2] <- "x5mins"


# changing timestamps to dates
df_date=df
for(i in 1:length(temp)){
  for(j in 1:length(df_date[[i]]$timestamp)){
    df_date[[i]]$timestamp = as.POSIXct(df_date[[i]]$timestamp ,origin = "1970-01-01",tz = "GMT")
  }
}

# appending dates to the new dataframe
cpup <- cbind(cpup, df_date[[1]]$timestamp[1:8600])
names(cpup)[3] <- "date"


```

```{r}
# TESTING FOR % CPU USAGE...

# polynomial Regression of order-3; Horrible results!
lm.fitcpup1=lm(avgCPUperc~poly(x5mins ,3),data=cpup)
plot(cpup$x5mins,cpup$avgCPUperc)
lines(cpup$x5mins, fitted(lm.fitcpup1), col='red')
new.data <- data.frame(x5mins = 9000)  # prediction for 33 hours in future
predict(lm.fitcpup1, newdata=new.data)



# Polynomial Regression of order-25; Horrible results! 
lm.fitcpup2=lm(avgCPUperc~poly(x5mins ,25),data=cpup)
plot(cpup$x5mins,cpup$avgCPUperc)
lines(cpup$x5mins, fitted(lm.fitcpup2), col='red')
new.data <- data.frame(x5mins = 9000) # prediction for 33 hours in future
predict(lm.fitcpup2, newdata=new.data)



# "Prophet" Model (by Facebook)
library(prophet)
library(dplyr)
library(ggplot2)
library(tseries)
library(forecast)

df1 = df

# Learning on 33.33% of training data
cpup1 <- cbind(cpup, df1[[1]]$timestamp[1:8600])
colnames(cpup1) <- c("y", "ds1", "ds")
cpup2 <- cpup1[1:2900,1:3]
m <- prophet(cpup2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)

# Learning on 50% of training data
cpup1 <- cbind(cpup, df1[[1]]$timestamp[1:8600])
colnames(cpup1) <- c("y", "ds1", "ds")
cpup2 <- cpup1[1:4300,1:3]
m <- prophet(cpup2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)

# Learning on 66.67% of training data
cpup1 <- cbind(cpup, df1[[1]]$timestamp[1:8600])
colnames(cpup1) <- c("y", "ds1", "ds")
cpup2 <- cpup1[1:5800,1:3]
m <- prophet(cpup2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)

# Learning on 100% of training data
cpup1 <- cbind(cpup, df1[[1]]$timestamp[1:8600])
colnames(cpup1) <- c("y", "ds1", "ds")
cpup2 <- cpup1[1:8600,1:3]
m <- prophet(cpup2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)


# ARIMA model
ARIMA_CPUusage <- cpup
  #ARIMA_CPUusage$date = as.Date(ARIMA_CPUusage$date)
plot(ARIMA_CPUusage$date, ARIMA_CPUusage$avgCPUperc)
  #ggplot(ARIMA_CPUusage, aes(ARIMA_CPUusage$date, ARIMA_CPUusage$avgCPUperc)) + geom_line() + scale_x_date('date')  + ylab("% CPU usage") + xlab("")


# tsclean() is used to clean the data and remove outliers
count_ts = ts(ARIMA_CPUusage[, c('avgCPUperc')])
ARIMA_CPUusage$clean_cnt = tsclean(count_ts)
ggplot() +
  geom_line(data = ARIMA_CPUusage, aes(x = date, y = clean_cnt)) + ylab('Cleaned % CPU usage')

#  using clean count with no outliers to calculate moving average of two different orders
ARIMA_CPUusage$cnt_ma = ma(ARIMA_CPUusage$clean_cnt, order=7)
ARIMA_CPUusage$cnt_ma30 = ma(ARIMA_CPUusage$clean_cnt, order=30) 

# plotting moving average over one another
ggplot() +
  geom_line(data = ARIMA_CPUusage, aes(x = date, y = clean_cnt, colour = "Counts")) +
  geom_line(data = ARIMA_CPUusage, aes(x = date, y = cnt_ma,   colour = "Weekly Moving Average"))  +
  geom_line(data = ARIMA_CPUusage, aes(x = date, y = cnt_ma30, colour = "Monthly Moving Average"))  +
  ylab('Cleaned % CPU Usage')

# Decomposing data into frequency of 4 as well as 30 for both orders of moving average
count_ma = ts(na.omit(ARIMA_CPUusage$cnt_ma), frequency=4)
decomp = stl(count_ma, s.window="periodic")
deseasonal_cnt <- seasadj(decomp)
plot(decomp)

count_ma = ts(na.omit(ARIMA_CPUusage$cnt_ma), frequency=30)
decomp = stl(count_ma, s.window="periodic")
deseasonal_cnt <- seasadj(decomp)
plot(decomp)

count_ma30 = ts(na.omit(ARIMA_CPUusage$cnt_ma30), frequency=4)
decomp30 = stl(count_ma30, s.window="periodic")
deseasonal_cnt30 <- seasadj(decomp30)
plot(decomp30)

count_ma30 = ts(na.omit(ARIMA_CPUusage$cnt_ma30), frequency=30)
decomp30 = stl(count_ma30, s.window="periodic")
deseasonal_cnt30 <- seasadj(decomp30)
plot(decomp30)

# Running Augmented Dickey-Fuller Test on the decomposed data for both orders of moving average
adf.test(count_ma, alternative = "stationary")
adf.test(count_ma30, alternative = "stationary")

##
# We are getting p-value < 0.01. Hence we accept our alternative hypothesis
##

# Plotting Autocorrelation Function for both orders of moving average
Acf(count_ma, main='')
Acf(count_ma30, main='')

# Plotting Partial Autocorrelation Function for both orders of moving average
Pacf(count_ma, main='')
Pacf(count_ma30, main='')

# In case our alternative hypothesis gets rejected (because of large p-value), we do differencing for d=1 times and make it stationary time-series (for both orders of moving average)
count_d1 = diff(deseasonal_cnt, differences = 1)
plot(count_d1)
adf.test(count_d1, alternative = "stationary")

count_d130 = diff(deseasonal_cnt30, differences = 1)
plot(count_d130)
adf.test(count_d1, alternative = "stationary")

# Re-plotting ACF and PACF with new differenced series for both orders of moving average
Acf(count_d1, main='ACF for Differenced Series')
Acf(count_d130, main='ACF for Differenced Series')
Pacf(count_d1, main='PACF for Differenced Series')
Pacf(count_d130, main='PACF for Differenced Series')

# Fitting ARIMA model for both orders of moving average
auto.arima(deseasonal_cnt, seasonal=FALSE)
auto.arima(deseasonal_cnt30, seasonal=FALSE)

# Plotting residuals from the ARIMA fitted model (for both orders of moving average)
fit <- auto.arima(deseasonal_cnt, seasonal=FALSE)
tsdisplay(residuals(fit), lag.max=60, main='(3,1,2) Model Residuals')
fit30 <- auto.arima(deseasonal_cnt30, seasonal=FALSE)
tsdisplay(residuals(fit30), lag.max=60, main='(3,1,5) Model Residuals')

# Creating seasonal ARIMA fitting; in case our model is "differenced" 
fit <- auto.arima(deseasonal_cnt, seasonal=TRUE)
tsdisplay(residuals(fit), lag.max=60, main='(3,1,2) Model Residuals')
fit30 <- auto.arima(deseasonal_cnt30, seasonal=TRUE)
tsdisplay(residuals(fit30), lag.max=60, main='(3,1,5) Model Residuals')

# Forecasting 75 hours into future for both orders of moving average
fcast <- forecast(fit, h=900)
plot(fcast)
fcast30 <- forecast(fit30, h=900)
plot(fcast30)

# Cross validating ARIMA model with training done on 2/3rd of data, and then predicting 75 hours into future (for both orders of moving average)
hold <- window(ts(deseasonal_cnt), start=4300)
fit_no_holdout = arima(ts(deseasonal_cnt[-c(5800:8600)]))
fcast_no_holdout <- forecast(fit_no_holdout,h=900)
plot(fcast_no_holdout, main=" ")
lines(ts(deseasonal_cnt))

hold30 <- window(ts(deseasonal_cnt30), start=4300)
fit_no_holdout30 = arima(ts(deseasonal_cnt30[-c(5800:8600)]))
fcast_no_holdout30 <- forecast(fit_no_holdout30, h=900)
plot(fcast_no_holdout30, main=" ")
lines(ts(deseasonal_cnt30))


```


```{r}
#2. NETWORK THROUGHPUT

# making a new Dataframe for TOTAL NETWORK THROUGHPUT and x5mins for training
tempavg2=avg2
nettp = data.frame(Reduce(rbind, tempavg2))
names(nettp) <- "networktp"
nettp <- cbind(nettp, 1:8600)
names(nettp)[2] <- "x5mins"


# appending dates to the new dataframe (same variable as before)
nettp <- cbind(nettp, df_date[[1]]$timestamp[1:8600])
names(nettp)[3] <- "date"


```

```{r}
# TESTING FOR NETWORK THROUGHPUT...

# Polynomial Regression of order-3; Horrible results!
lm.fitnettp1=lm(networktp~poly(x5mins, 3), data=nettp)
plot(nettp$x5mins,nettp$networktp)
lines(nettp$x5mins, fitted(lm.fitnettp1), col='green')
new.data <- data.frame(x5mins = 9000)  # prediction for 33 hours in future
predict(lm.fitnettp1, newdata=new.data)



# Polynomial Regression of order-25; Horrible results! 
lm.fitnettp2=lm(networktp~poly(x5mins, 25), data=nettp)
plot(nettp$x5mins, nettp$networktp)
lines(nettp$x5mins, fitted(lm.fitnettp2), col='green')
new.data <- data.frame(x5mins = 9000) # prediction for 33 hours in future
predict(lm.fitnettp2, newdata=new.data)



# "Prophet" Model (by Facebook)

# Learning on 33.33% of training data
nettp1 <- cbind(nettp, df1[[1]]$timestamp[1:8600])
colnames(nettp1) <- c("y", "ds1", "ds")
nettp2 <- nettp1[1:2900,1:3]
m <- prophet(nettp2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)

# Learning on 50% of training data
nettp1 <- cbind(nettp, df1[[1]]$timestamp[1:8600])
colnames(nettp1) <- c("y", "ds1", "ds")
nettp2 <- nettp1[1:4300,1:3]
m <- prophet(nettp2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)

# Learning on 66.67% of training data
nettp1 <- cbind(nettp, df1[[1]]$timestamp[1:8600])
colnames(nettp1) <- c("y", "ds1", "ds")
nettp2 <- nettp1[1:5800,1:3]
m <- prophet(nettp2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)

# Learning on 100% of training data
nettp1 <- cbind(nettp, df1[[1]]$timestamp[1:8600])
colnames(nettp1) <- c("y", "ds1", "ds")
nettp2 <- nettp1[1:8600,1:3]
m <- prophet(nettp2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)


# ARIMA model
ARIMA_nettp <- nettp
  # ARIMA_nettp$date = as.Date(ARIMA_nettp$date)
plot(ARIMA_nettp$date, ARIMA_nettp$networktp)
  # ggplot(ARIMA_nettp, aes(ARIMA_nettp$date, ARIMA_nettp$networktp)) + geom_line() + scale_x_date('date')  + ylab("% memory usage") + xlab("")

# tsclean() is used to clean the data and remove outliers
count_ts = ts(ARIMA_nettp[, c('networktp')])
ARIMA_nettp$clean_cnt = tsclean(count_ts)
ggplot() +
  geom_line(data = ARIMA_nettp, aes(x = date, y = clean_cnt)) + ylab('Cleaned network tp')

#  using clean count with no outliers to calculate moving average of two different orders
ARIMA_nettp$cnt_ma = ma(ARIMA_nettp$clean_cnt, order=7) 
ARIMA_nettp$cnt_ma30 = ma(ARIMA_nettp$clean_cnt, order=30) 

# plotting moving average over one another
ggplot() +
  geom_line(data = ARIMA_nettp, aes(x = date, y = clean_cnt, colour = "Counts")) +
  geom_line(data = ARIMA_nettp, aes(x = date, y = cnt_ma,  colour = "Weekly Moving Average"))  +
  geom_line(data = ARIMA_nettp, aes(x = date, y = cnt_ma30, colour = "Monthly Moving Average"))  +
  ylab('Cleaned network tp')

# Decomposing data into frequency of 4 as well as 30 for both orders of moving average
count_ma = ts(na.omit(ARIMA_nettp$cnt_ma), frequency=4)
decomp = stl(count_ma, s.window="periodic")
deseasonal_cnt <- seasadj(decomp)
plot(decomp)

count_ma = ts(na.omit(ARIMA_nettp$cnt_ma), frequency=30)
decomp = stl(count_ma, s.window="periodic")
deseasonal_cnt <- seasadj(decomp)
plot(decomp)

count_ma30 = ts(na.omit(ARIMA_nettp$cnt_ma30), frequency=4)
decomp30 = stl(count_ma30, s.window="periodic")
deseasonal_cnt30 <- seasadj(decomp30)
plot(decomp30)

count_ma30 = ts(na.omit(ARIMA_nettp$cnt_ma30), frequency=30)
decomp30 = stl(count_ma30, s.window="periodic")
deseasonal_cnt30 <- seasadj(decomp30)
plot(decomp30)

# Running Augmented Dickey-Fuller Test on the decomposed data for both orders of moving average
adf.test(count_ma, alternative = "stationary")
adf.test(count_ma30, alternative = "stationary")

##
# We are getting p-value < 0.01. Hence we accept our alternative hypothesis
##

# Plotting Autocorrelation Function for both orders of moving average
Acf(count_ma, main='')
Acf(count_ma30, main='')

# Plotting Partial Autocorrelation Function for both orders of moving average
Pacf(count_ma, main='')
Pacf(count_ma30, main='')

# In case our alternative hypothesis gets rejected (because of large p-value), we do differencing for d=1 times and make it stationary time-series (for both orders of moving average)
count_d1 = diff(deseasonal_cnt, differences = 1)
plot(count_d1)
adf.test(count_d1, alternative = "stationary")

count_d130 = diff(deseasonal_cnt30, differences = 1)
plot(count_d130)
adf.test(count_d1, alternative = "stationary")

# Re-plotting ACF and PACF with new differenced series for both orders of moving average
Acf(count_d1, main='ACF for Differenced Series')
Acf(count_d130, main='ACF for Differenced Series')
Pacf(count_d1, main='PACF for Differenced Series')
Pacf(count_d130, main='PACF for Differenced Series')

# Fitting ARIMA model for both orders of moving average
auto.arima(deseasonal_cnt, seasonal=FALSE)
auto.arima(deseasonal_cnt30, seasonal=FALSE)

# Plotting residuals from the ARIMA fitted model (for both orders of moving average)
fit <- auto.arima(deseasonal_cnt, seasonal=FALSE)
tsdisplay(residuals(fit), lag.max=60, main='(3,1,3) Model Residuals')
fit30 <- auto.arima(deseasonal_cnt30, seasonal=FALSE)
tsdisplay(residuals(fit30), lag.max=60, main='(1,1,0) Model Residuals')

# Creating seasonal ARIMA fitting; in case our model is "differenced" 
fit <- auto.arima(deseasonal_cnt, seasonal=TRUE)
tsdisplay(residuals(fit), lag.max=60, main='(3,1,3) Model Residuals')
fit30 <- auto.arima(deseasonal_cnt30, seasonal=TRUE)
tsdisplay(residuals(fit30), lag.max=60, main='(1,1,0) Model Residuals')

# Forecasting 75 hours into future for both orders of moving average
fcast <- forecast(fit, h=900)
plot(fcast)
fcast30 <- forecast(fit30, h=900)
plot(fcast30)

# Cross validating ARIMA model with training done on 2/3rd of data, and then predicting 75 hours into future (for both orders of moving average)
hold <- window(ts(deseasonal_cnt), start=4300)
fit_no_holdout = arima(ts(deseasonal_cnt[-c(5800:8600)]))
fcast_no_holdout <- forecast(fit_no_holdout,h=900)
plot(fcast_no_holdout, main=" ")
lines(ts(deseasonal_cnt))

hold30 <- window(ts(deseasonal_cnt30), start=4300)
fit_no_holdout30 = arima(ts(deseasonal_cnt30[-c(5800:8600)]))
fcast_no_holdout30 <- forecast(fit_no_holdout30, h=900)
plot(fcast_no_holdout30, main=" ")
lines(ts(deseasonal_cnt30))


```

```{r}
#3. DISK THROUGHPUT

# making a new Dataframe for TOTAL NETWORK THROUGHPUT and x5mins for training
tempavg3=avg3
disktp = data.frame(Reduce(rbind, tempavg3))
names(disktp) <- "disktp"
disktp <- cbind(disktp, 1:8600)
names(disktp)[2] <- "x5mins"


# appending dates to the new dataframe (same variable as before)
disktp <- cbind(disktp, df_date[[1]]$timestamp[1:8600])
names(disktp)[3] <- "date"


```


```{r}

# TESTING FOR DISK THROUGHPUT...

# Polynomial Regression of order-3; Horrible results!
lm.fitdisktp1=lm(disktp~poly(x5mins, 3), data=disktp)
plot(disktp$x5mins, disktp$disktp)
lines(disktp$x5mins, fitted(lm.fitdisktp1), col='blue')
new.data <- data.frame(x5mins = 9000)  # prediction for 33 hours in future
predict(lm.fitdisktp1, newdata=new.data)



# Polynomial Regression of order-25; Horrible results! 
lm.fitdisktp2=lm(disktp~poly(x5mins, 25), data=disktp)
plot(disktp$x5mins, disktp$disktp)
lines(disktp$x5mins, fitted(lm.fitdisktp2), col='blue')
new.data <- data.frame(x5mins = 9000) # prediction for 33 hours in future
predict(lm.fitdisktp2, newdata=new.data)



# "Prophet" Model (by Facebook)

# Learning on 33.33% of training data
disktp1 <- cbind(disktp, df1[[1]]$timestamp[1:8600])
colnames(disktp1) <- c("y", "ds1", "ds")
disktp2 <- disktp1[1:2900,1:3]
m <- prophet(disktp2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)

# Learning on 50% of training data
disktp1 <- cbind(disktp, df1[[1]]$timestamp[1:8600])
colnames(disktp1) <- c("y", "ds1", "ds")
disktp2 <- disktp1[1:4300,1:3]
m <- prophet(disktp2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)

# Learning on 66.67% of training data
disktp1 <- cbind(disktp, df1[[1]]$timestamp[1:8600])
colnames(disktp1) <- c("y", "ds1", "ds")
disktp2 <- disktp1[1:5800,1:3]
m <- prophet(disktp2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)

# Learning on 100% of training data
disktp1 <- cbind(disktp, df1[[1]]$timestamp[1:8600])
colnames(disktp1) <- c("y", "ds1", "ds")
disktp2 <- disktp1[1:8600,1:3]
m <- prophet(disktp2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)


# ARIMA model
ARIMA_disktp <- disktp
  # ARIMA_disktp$date = as.Date(ARIMA_disktp$date)
plot(ARIMA_disktp$date, ARIMA_disktp$disktp)
  # ggplot(ARIMA_disktp, aes(ARIMA_disktp$date, ARIMA_disktp$disktp)) + geom_line() + scale_x_date('date')  + ylab("% memory usage") + xlab("")

# tsclean() is used to clean the data and remove outliers
count_ts = ts(ARIMA_disktp[, c('disktp')])
ARIMA_disktp$clean_cnt = tsclean(count_ts)
ggplot() +
  geom_line(data = ARIMA_disktp, aes(x = date, y = clean_cnt)) + ylab('Cleaned disk tp')

#  using clean count with no outliers to calculate moving average of two different orders
ARIMA_disktp$cnt_ma = ma(ARIMA_disktp$clean_cnt, order=7) 
ARIMA_disktp$cnt_ma30 = ma(ARIMA_disktp$clean_cnt, order=30) 

# plotting moving average over one another
ggplot() +
  geom_line(data = ARIMA_disktp, aes(x = date, y = clean_cnt, colour = "Counts")) +
  geom_line(data = ARIMA_disktp, aes(x = date, y = cnt_ma,  colour = "Weekly Moving Average"))  +
  geom_line(data = ARIMA_disktp, aes(x = date, y = cnt_ma30, colour = "Monthly Moving Average"))  +
  ylab('Cleaned disk tp')

# Decomposing data into frequency of 4 as well as 30 for both orders of moving average
count_ma = ts(na.omit(ARIMA_disktp$cnt_ma), frequency=4)
decomp = stl(count_ma, s.window="periodic")
deseasonal_cnt <- seasadj(decomp)
plot(decomp)

count_ma = ts(na.omit(ARIMA_disktp$cnt_ma), frequency=30)
decomp = stl(count_ma, s.window="periodic")
deseasonal_cnt <- seasadj(decomp)
plot(decomp)

count_ma30 = ts(na.omit(ARIMA_disktp$cnt_ma30), frequency=4)
decomp30 = stl(count_ma30, s.window="periodic")
deseasonal_cnt30 <- seasadj(decomp30)
plot(decomp30)

count_ma30 = ts(na.omit(ARIMA_disktp$cnt_ma30), frequency=30)
decomp30 = stl(count_ma30, s.window="periodic")
deseasonal_cnt30 <- seasadj(decomp30)
plot(decomp30)

# Running Augmented Dickey-Fuller Test on the decomposed data for both orders of moving average
adf.test(count_ma, alternative = "stationary")
adf.test(count_ma30, alternative = "stationary")

##
# We are getting p-value < 0.01. Hence we accept our alternative hypothesis
##

# Plotting Autocorrelation Function for both orders of moving average
Acf(count_ma, main='')
Acf(count_ma30, main='')

# Plotting Partial Autocorrelation Function for both orders of moving average
Pacf(count_ma, main='')
Pacf(count_ma30, main='')

# In case our alternative hypothesis gets rejected (because of large p-value), we do differencing for d=1 times and make it stationary time-series (for both orders of moving average)
count_d1 = diff(deseasonal_cnt, differences = 1)
plot(count_d1)
adf.test(count_d1, alternative = "stationary")

count_d130 = diff(deseasonal_cnt30, differences = 1)
plot(count_d130)
adf.test(count_d1, alternative = "stationary")

# Re-plotting ACF and PACF with new differenced series for both orders of moving average
Acf(count_d1, main='ACF for Differenced Series')
Acf(count_d130, main='ACF for Differenced Series')
Pacf(count_d1, main='PACF for Differenced Series')
Pacf(count_d130, main='PACF for Differenced Series')

# Fitting ARIMA model for both orders of moving average
auto.arima(deseasonal_cnt, seasonal=FALSE)
auto.arima(deseasonal_cnt30, seasonal=FALSE)

# Plotting residuals from the ARIMA fitted model (for both orders of moving average)
fit <- auto.arima(deseasonal_cnt, seasonal=FALSE)
tsdisplay(residuals(fit), lag.max=60, main='(1,1,0) Model Residuals')
fit30 <- auto.arima(deseasonal_cnt30, seasonal=FALSE)
tsdisplay(residuals(fit30), lag.max=60, main='(5,1,0) Model Residuals')

# Creating seasonal ARIMA fitting; in case our model is "differenced" 
fit <- auto.arima(deseasonal_cnt, seasonal=TRUE)
tsdisplay(residuals(fit), lag.max=60, main='(1,1,0) Model Residuals')
fit30 <- auto.arima(deseasonal_cnt30, seasonal=TRUE)
tsdisplay(residuals(fit30), lag.max=60, main='(5,1,0) Model Residuals')

# Forecasting 75 hours into future for both orders of moving average
fcast <- forecast(fit, h=900)
plot(fcast)
fcast30 <- forecast(fit30, h=900)
plot(fcast30)

# Cross validating ARIMA model with training done on 2/3rd of data, and then predicting 75 hours into future (for both orders of moving average)
hold <- window(ts(deseasonal_cnt), start=4300)
fit_no_holdout = arima(ts(deseasonal_cnt[-c(5800:8600)]))
fcast_no_holdout <- forecast(fit_no_holdout,h=900)
plot(fcast_no_holdout, main=" ")
lines(ts(deseasonal_cnt))

hold30 <- window(ts(deseasonal_cnt30), start=4300)
fit_no_holdout30 = arima(ts(deseasonal_cnt30[-c(5800:8600)]))
fcast_no_holdout30 <- forecast(fit_no_holdout30, h=900)
plot(fcast_no_holdout30, main=" ")
lines(ts(deseasonal_cnt30))


```



```{r}
#4. % MEMORY USAGE

# making a new Dataframe for % MEMORY USAGE and x5mins for training
tempavg4=avg4
memp = data.frame(Reduce(rbind, tempavg4))
names(memp) <- "avgmemperc"
memp <- cbind(memp, 1:8600)
names(memp)[2] <- "x5mins"


# appending dates to the new dataframe
memp <- cbind(memp, df_date[[1]]$timestamp[1:8600])
names(memp)[3] <- "date"

```



```{r}
# TESTING FOR % Memory USAGE...

# Wolynomial Regression of order-3; Horrible results!
lm.fitmemp1=lm(avgmemperc~poly(x5mins ,3),data=memp)
plot(memp$x5mins,memp$avgmemperc)
lines(memp$x5mins, fitted(lm.fitmemp1), col='red')
new.data <- data.frame(x5mins = 9000)  # prediction for 33 hours in future
predict(lm.fitmemp1, newdata=new.data)



# Polynomial Regression of order-25; Horrible results! 
lm.fitmemp2=lm(avgmemperc~poly(x5mins ,25),data=memp)
plot(memp$x5mins,memp$avgmemperc)
lines(memp$x5mins, fitted(lm.fitmemp2), col='red')
new.data <- data.frame(x5mins = 9000) # prediction for 33 hours in future
predict(lm.fitmemp2, newdata=new.data)



# "Prophet" Model (by Facebook)

# Learning on 33.33% of training data
memp1 <- cbind(memp, df1[[1]]$timestamp[1:8600])
colnames(memp1) <- c("y", "ds1", "ds")
memp2 <- memp1[1:2900,1:3]
m <- prophet(memp2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)

# Learning on 50% of training data
memp1 <- cbind(memp, df1[[1]]$timestamp[1:8600])
colnames(memp1) <- c("y", "ds1", "ds")
memp2 <- memp1[1:4300,1:3]
m <- prophet(memp2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)

# Learning on 66.67% of training data
memp1 <- cbind(memp, df1[[1]]$timestamp[1:8600])
colnames(memp1) <- c("y", "ds1", "ds")
memp2 <- memp1[1:5800,1:3]
m <- prophet(memp2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)

# Learning on 100% of training data
memp1 <- cbind(memp, df1[[1]]$timestamp[1:8600])
colnames(memp1) <- c("y", "ds1", "ds")
memp2 <- memp1[1:8600,1:3]
m <- prophet(memp2)
future <- make_future_dataframe(m, periods = 14,freq = 'day')
forecast <- predict(m, future)
plot(m, forecast)


# ARIMA model
ARIMA_memusage <- memp
  #ARIMA_memusage$date = as.Date(ARIMA_memusage$date)
plot(ARIMA_memusage$date, ARIMA_memusage$avgmemperc)
  # ggplot(ARIMA_memusage, aes(ARIMA_memusage$date, ARIMA_memusage$avgmemperc)) + geom_line() + scale_x_date('date')  + ylab("% Memory usage") + xlab("")

# tsclean() is used to clean the data and remove outliers
count_ts = ts(ARIMA_memusage[, c('avgmemperc')])
ARIMA_memusage$clean_cnt = tsclean(count_ts)
ggplot() +
  geom_line(data = ARIMA_memusage, aes(x = date, y = clean_cnt)) + ylab('Cleaned % Memory usage')

#  using clean count with no outliers to calculate moving average of two different orders
ARIMA_memusage$cnt_ma = ma(ARIMA_memusage$clean_cnt, order=7) 
ARIMA_memusage$cnt_ma30 = ma(ARIMA_memusage$clean_cnt, order=30) 

# plotting moving average over one another
ggplot() +
  geom_line(data = ARIMA_memusage, aes(x = date, y = clean_cnt, colour = "Counts")) +
  geom_line(data = ARIMA_memusage, aes(x = date, y = cnt_ma,   colour = "Weekly Moving Average"))  +
  geom_line(data = ARIMA_memusage, aes(x = date, y = cnt_ma30, colour = "Monthly Moving Average"))  +
  ylab('Cleaned % Memory Usage')

# Decomposing data into frequency of 4 as well as 30 for both orders of moving average
count_ma = ts(na.omit(ARIMA_memusage$cnt_ma), frequency=4)
decomp = stl(count_ma, s.window="periodic")
deseasonal_cnt <- seasadj(decomp)
plot(decomp)

count_ma = ts(na.omit(ARIMA_memusage$cnt_ma), frequency=30)
decomp = stl(count_ma, s.window="periodic")
deseasonal_cnt <- seasadj(decomp)
plot(decomp)

count_ma30 = ts(na.omit(ARIMA_memusage$cnt_ma30), frequency=4)
decomp30 = stl(count_ma30, s.window="periodic")
deseasonal_cnt30 <- seasadj(decomp30)
plot(decomp30)

count_ma30 = ts(na.omit(ARIMA_memusage$cnt_ma30), frequency=30)
decomp30 = stl(count_ma30, s.window="periodic")
deseasonal_cnt30 <- seasadj(decomp30)
plot(decomp30)

# Running Augmented Dickey-Fuller Test on the decomposed data for both orders of moving average
adf.test(count_ma, alternative = "stationary")
adf.test(count_ma30, alternative = "stationary")

##
# We are getting p-value < 0.01. Hence we accept our alternative hypothesis
##

# Plotting Autocorrelation Function for both orders of moving average
Acf(count_ma, main='')
Acf(count_ma30, main='')

# Plotting Partial Autocorrelation Function for both orders of moving average
Pacf(count_ma, main='')
Pacf(count_ma30, main='')

# In case our alternative hypothesis gets rejected (because of large p-value), we do differencing for d=1 times and make it stationary time-series (for both orders of moving average)
count_d1 = diff(deseasonal_cnt, differences = 1)
plot(count_d1)
adf.test(count_d1, alternative = "stationary")

count_d130 = diff(deseasonal_cnt30, differences = 1)
plot(count_d130)
adf.test(count_d1, alternative = "stationary")

# Re-plotting ACF and PACF with new differenced series for both orders of moving average
Acf(count_d1, main='ACF for Differenced Series')
Acf(count_d130, main='ACF for Differenced Series')
Pacf(count_d1, main='PACF for Differenced Series')
Pacf(count_d130, main='PACF for Differenced Series')

# Fitting ARIMA model for both orders of moving average
auto.arima(deseasonal_cnt, seasonal=FALSE)
auto.arima(deseasonal_cnt30, seasonal=FALSE)

# Plotting residuals from the ARIMA fitted model (for both orders of moving average)
fit <- auto.arima(deseasonal_cnt, seasonal=FALSE)
tsdisplay(residuals(fit), lag.max=60, main='(3,1,2) Model Residuals')
fit30 <- auto.arima(deseasonal_cnt30, seasonal=FALSE)
tsdisplay(residuals(fit30), lag.max=60, main='(3,1,5) Model Residuals')

# Creating seasonal ARIMA fitting; in case our model is "differenced" 
fit <- auto.arima(deseasonal_cnt, seasonal=TRUE)
tsdisplay(residuals(fit), lag.max=60, main='(3,1,2) Model Residuals')
fit30 <- auto.arima(deseasonal_cnt30, seasonal=TRUE)
tsdisplay(residuals(fit30), lag.max=60, main='(3,1,5) Model Residuals')

# Forecasting 75 hours into future for both orders of moving average
fcast <- forecast(fit, h=900)
plot(fcast)
fcast30 <- forecast(fit30, h=900)
plot(fcast30)

# Cross validating ARIMA model with training done on 2/3rd of data, and then predicting 75 hours into future (for both orders of moving average)
hold <- window(ts(deseasonal_cnt), start=4300)
fit_no_holdout = arima(ts(deseasonal_cnt[-c(5800:8600)]))
fcast_no_holdout <- forecast(fit_no_holdout,h=900)
plot(fcast_no_holdout, main=" ")
lines(ts(deseasonal_cnt))

hold30 <- window(ts(deseasonal_cnt30), start=4300)
fit_no_holdout30 = arima(ts(deseasonal_cnt30[-c(5800:8600)]))
fcast_no_holdout30 <- forecast(fit_no_holdout30, h=900)
plot(fcast_no_holdout30, main=" ")
lines(ts(deseasonal_cnt30))


```

